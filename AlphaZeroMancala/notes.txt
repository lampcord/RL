MCTS Results:
Rounds   FastRollout   NumRollout  Loops  Time  Wins Duration
Fast rollout * 1000 vs standard rollout. Definite edge to FR.
1000     True          1000        10000  1.0   563  12804.7408
1000     False         1           10000  1.0   422  12730.2601
Surprising that going second plays better. And suspicious.
Turn 0  391
Turn 1  594

Fast rollout * 1 vs standard rollout. Even bigger edge. In far less time
1000     True          1           10000  1.0   622   4580.8118
1000     False         1           10000  1.0   349  12420.9238
Closer, but still, why does going second win more. Should be 60/40 is other way.
Turn 0  474
Turn 1  497

Surprisingly, FR * 1000 dominates FR * 1 (Rock / Paper / Scissors)?
Looks like the FR * 1 run was able to easily saturate the 10k limit.
1000     True          1000        10000  1.0   640  12517.9875
1000     True          1           10000  1.0   357   4881.0365
Even bigger discrepancy.
Turn 0  242
Turn 1  755

Trying again with loops set to 1m. See if that gives edge back to FR * 1
----------------------------------------------------------------------------------------
MCTSAgent 0
loops:1000000 c:1.14 most_visits:True rollout_policy:True rollout_count:1000 max_time:1.0
MCTSAgent 1
loops:1000000 c:1.14 most_visits:True rollout_policy:True rollout_count:1 max_time:1.0
----------------------------------------------------------------------------------------
1000     True          1000        1000000  1.0   119  12697.8602
1000     True          1           1000000  1.0   850  13049.7806
----------------------------------------------------------------------------------------
Turn 0  444
Turn 1  525
Now that's more like it! Now that the time is more or less equalized by using more loops than
can be saturated by the 1 second time limit, the extra tree expansion allowed by doing only 1 simulation
dominates the increased accuracy of the 1000 simulations.

TODO:
[X] Create a configuration class for MCTS with defaults, command line is getting out of hand!
[X] Agent descriptions
[/] Memory
[/] AB enhancements
[ ] Time management enhancements (total time instead of per turn)


Thoughts on training and using trained memory.

Analyze: Since we are going to fix the table to a specific size, would it make more sense to do the following:
    1) Expand the memory as before in (A).
    2) Once we hit the leaf cap, simply cycle through all of the leafs and saturate them to the visit cap.
    3) Also, should we make the original tree much larger than the target cap (10x?) and then trow away the least visited since those are likely not used much in a competitive game?
    4) What about tracking unexpected moves that lead to an oponent's victory?

A) Learn mode.
    We start with blank memory and then we play 2 agents against each other using a relatively large rollout (100-1000)
    We save after each session. Loading at the start of new sessions.
    We apply the following constraints:
        1) We put a cap on the total number of leafs (1m?)
        2) We put a cap on the max visits for a leaf (1m?)

B) Once we have it saturated we go to play mode.
    We run in the 1 rollout mode since we have seen this to be the best.
    In this mode, we no longer update or save the memory. Instead, we check for the existence of a leaf in memory.
    If it is there, we use it. Otherwise, we do a traditional rollout.

This gives us the advantages of more accurate rollout evaluation in the early game without losing the advantage of tree
expansion from using the 1 rollout policy.

Enhancements:
    1) Many positions will be mirrored from player A to player B. We can check for mirrors by reversing the position and then choosing the one with the smaller binary number.
    This will have a small tradeoff of initial lookup speed with doubling the amount of positions in the same number of leafs.

    2) Similarly, we can mirror positions vertically, cutting space in half again.

    3) Once we have failed a lookup in a single game, we no longer have to perform any more lookups because any new position in memory would have to have had a parent previously.

What we need:
    A way to specify the filename.
    A way to specify max leaf count.
    A way to specify max visits
    A flag to indicate that we are playing in learn mode.
    A flag to indicate player mirroring (so we can validate against previous version).
    A flag to indicate vertical mirroring.
    To track that we have already missed a leaf for a given player, we need to indicate that we are starting a game to clear these flags.


-----------------------------------------------------------------
On the finalize part:
Clear out any prior info.
At the start of the rollout, do a N level minmax
Results are {win, unknown, loss}
If there is a forced win move, report back a 1 and we are done searching this position.
else eliminate all loss moves from consideration. If no moves left, report a 0 and we are done searching this position.
If there are any more moves left, do normal rollouts but only use those moves.
-----------------------------------------------------------------

Adding beam search
While adding min_max to the rollout process directly does not increase the performance, there are other ways to use this by adding a new file, the beam file.
There are four outcomes from a mm search:
1) Forced Win
2) Forced Loss
3) Some moves lead to a loss.
4) No changes.

The beam table needs 2 pieces of information:
1) What is the outcome of the position (player 0 win, player 1 win, tie, not completed)
2) What is the list of legal moves. These could be either the original legal moves or the pruned legal moves.

We first create the beam table.

Then we go through all of the positions and use the beam table to prune moves and truncate searches.
If the initial position is 1,2 we simply record the score and move on.
If the initial position is 3 we use the beam searches move list and rollout as normal.

-----------------------------------------------------------------
Would it be faster to cache key -> array_pos, player for all keys in score map?